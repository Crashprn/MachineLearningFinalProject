{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from NNs import NeuralNetwork\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2         3         4         5         6  \\\n",
      "0 -0.706173 -0.479326  0.695775  0.105923  0.223339 -0.020102  0.166236   \n",
      "1 -0.292061 -0.203339 -0.012935 -0.440923  0.521172  0.085015  0.007540   \n",
      "2 -0.131100  0.053226 -0.548904 -0.121370  0.658592 -0.089726 -0.297121   \n",
      "3  0.128701  0.005418  0.173510 -0.634204  0.039833  0.456838 -0.580948   \n",
      "4 -0.220905 -0.434828 -0.033333 -0.451370  0.842869  0.286226 -0.604444   \n",
      "\n",
      "          7         8         9  ...       760       761       762       763  \\\n",
      "0 -0.118495  0.227647 -0.517054  ... -0.092164  0.139747 -0.496852  0.922823   \n",
      "1 -0.240759 -0.673398  0.952635  ... -0.586681 -0.748257 -0.970952  0.007169   \n",
      "2 -0.192829 -0.740907 -0.001359  ...  0.929475 -0.510744 -0.698830  0.517608   \n",
      "3 -0.463875 -0.434695  0.994325  ... -0.331720 -0.740078 -0.848269  0.427743   \n",
      "4 -0.219417 -0.062626  0.475039  ...  0.892794 -0.146139 -0.736430  0.861435   \n",
      "\n",
      "        764       765       766       767  target  \\\n",
      "0 -0.391260  0.058642 -0.055115  0.239891       1   \n",
      "1  0.078394 -0.167922  0.490578 -0.303030       1   \n",
      "2  0.511074 -0.564630  0.319537  0.190828       1   \n",
      "3  0.124267  0.111930  0.655725 -0.640740       1   \n",
      "4 -0.358414 -0.623778  0.091531 -0.047541       1   \n",
      "\n",
      "                                                text  \n",
      "0  Our Deeds are the Reason of this #earthquake M...  \n",
      "1             Forest fire near La Ronge Sask. Canada  \n",
      "2  All residents asked to 'shelter in place' are ...  \n",
      "3  13,000 people receive #wildfires evacuation or...  \n",
      "4  Just got sent this photo from Ruby #Alaska as ...  \n",
      "\n",
      "[5 rows x 770 columns]\n"
     ]
    }
   ],
   "source": [
    "#df = pd.read_csv('data/processed_bert.csv')\n",
    "df = pd.read_csv('data/processed_fine_encode4_pool_bert.csv')\n",
    "#df = pd.read_csv('data/processed_fine_pool_bert.csv')\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "data = df[[str(i) for i in range(768)]+[\"target\"]].values\n",
    "\n",
    "text_embeddings = data[:,:-1]\n",
    "targets = data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive samples: 3271.0\n",
      "Number of negative samples: 4342.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "print(f\"Number of positive samples: {np.sum(targets)}\")\n",
    "print(f\"Number of negative samples: {len(targets) - np.sum(targets)}\")\n",
    "\n",
    "x = text_embeddings\n",
    "y = targets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation accuracy: 0.9059525298899139\n",
      "Best LR: {'C': 0.1}\n",
      "F1 Score: 0.8948545861297538\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "gs = GridSearchCV(LogisticRegression(max_iter=1000, n_jobs=2), param_grid={'C': [0.1]}, cv=5, n_jobs=8)\n",
    "\n",
    "gs = gs.fit(x, y)\n",
    "\n",
    "best_lr = gs.best_estimator_\n",
    "\n",
    "cv_acc = gs.best_score_\n",
    "\n",
    "LR_predictions = best_lr.predict(x)\n",
    "\n",
    "print(f'Cross Validation accuracy: {cv_acc}')\n",
    "print(f'Best LR: {gs.best_params_}')\n",
    "print(f'F1 Score: {f1_score(y, LR_predictions)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation accuracy: 0.9016170795071281\n",
      "Best RF params: {'n_estimators': 300}\n",
      "F1 Score: 0.99663505659223\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "gs = GridSearchCV(RandomForestClassifier(n_jobs=2), param_grid={'n_estimators': [300]}, cv=5, n_jobs=8)\n",
    "gs = gs.fit(x, y)\n",
    "\n",
    "best_rf = gs.best_estimator_\n",
    "\n",
    "cv_acc = gs.best_score_\n",
    "\n",
    "RF_predictions = best_rf.predict(x)\n",
    "\n",
    "print(f'Cross Validation accuracy: {cv_acc}')\n",
    "print(f'Best RF params: {gs.best_params_}')\n",
    "print(f'F1 Score: {f1_score(y, RF_predictions)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation accuracy: 0.9027993025039625\n",
      "Best SVM: {'C': 5}\n",
      "F1 Score: 0.8960411972964274\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "gs = GridSearchCV(SVC(), param_grid={'C': [5]}, cv=5, n_jobs=8)\n",
    "\n",
    "gs = gs.fit(x, y)\n",
    "\n",
    "best_svm = gs.best_estimator_\n",
    "\n",
    "cv_acc = gs.best_score_\n",
    "\n",
    "SVM_predictions = best_svm.predict(x)\n",
    "\n",
    "print(f'Cross Validation accuracy: {cv_acc}')\n",
    "print(f\"Best SVM: {gs.best_params_}\")\n",
    "print(f'F1 Score: {f1_score(y, SVM_predictions)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation accuracy: 0.8907122759820293\n",
      "Validation accuracy: 0.9061063690085358\n",
      "F1 Score: 0.8884989631520178\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "import torch\n",
    "import skorch\n",
    "\n",
    "from skorch.callbacks import EarlyStopping, Checkpoint\n",
    "\n",
    "num_epochs = 100\n",
    "checkpoint_dir = 'checkpoints/'\n",
    "\n",
    "callbacks = [EarlyStopping(patience=20, threshold=0.001, threshold_mode='abs', monitor='valid_acc', lower_is_better=False),\n",
    "            Checkpoint(monitor='valid_acc_best', f_params='DisasterClassifier.pt', dirname=checkpoint_dir)\n",
    "]\n",
    "\n",
    "net = skorch.NeuralNetBinaryClassifier(\n",
    "    NeuralNetwork,\n",
    "    module__input_size=768,\n",
    "    module__hidden_size=100,\n",
    "    module__output_size=1,\n",
    "    module__num_layers=6,\n",
    "    optimizer=Adam, \n",
    "    optimizer__weight_decay=0.00001,\n",
    "    lr=0.0001,\n",
    "    max_epochs=num_epochs, \n",
    "    batch_size=32, \n",
    "    device=torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'),\n",
    "    callbacks=callbacks,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "gs = GridSearchCV(net, param_grid={'lr': [0.001]}, cv=5, n_jobs=8)\n",
    "\n",
    "gs = gs.fit(x.astype(np.float32), y.astype(np.float32))\n",
    "\n",
    "cv_acc = gs.best_score_\n",
    "\n",
    "best_net = gs.best_estimator_\n",
    "\n",
    "best_net.load_params(f_params=checkpoint_dir+'DisasterClassifier.pt', f_optimizer=checkpoint_dir+'optimizer.pt', f_history=checkpoint_dir+'history.json')\n",
    "\n",
    "NN_predictions = best_net.predict(text_embeddings.astype(np.float32))\n",
    "\n",
    "print(f'Cross Validation accuracy: {cv_acc}')\n",
    "print(f'Validation accuracy: {best_net.history[-1][\"valid_acc\"]}')\n",
    "print(f'F1 Score: {f1_score(y, NN_predictions)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Common Incorrect Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mmmmmm I'm burning.... I'm burning buildings I'm building.... Oooooohhhh oooh ooh...\n",
      "LR: 0.0, RF: 0.0, SVM: 0.0, NN: 0, Target: 1.0\n",
      "like for the music video I want some real action shit like burning buildings and police chases not some weak ben winston shit\n",
      "LR: 0.0, RF: 0.0, SVM: 0.0, NN: 0, Target: 1.0\n",
      ".POTUS #StrategicPatience is a strategy for #Genocide; refugees; IDP Internally displaced people; horror; etc. https://t.co/rqWuoy1fm4\n",
      "LR: 1.0, RF: 1.0, SVM: 1.0, NN: 1, Target: 0.0\n",
      "He came to a land which was engulfed in tribal war and turned it into a land of peace i.e. Madinah. #ProphetMuhammad #islam\n",
      "LR: 0.0, RF: 0.0, SVM: 0.0, NN: 0, Target: 1.0\n",
      "He came to a land which was engulfed in tribal war and turned it into a land of peace i.e. Madinah. #ProphetMuhammad #islam\n",
      "LR: 0.0, RF: 0.0, SVM: 0.0, NN: 0, Target: 1.0\n",
      "Caution: breathing may be hazardous to your health.\n",
      "LR: 0.0, RF: 0.0, SVM: 0.0, NN: 0, Target: 1.0\n",
      "The Prophet (peace be upon him) said 'Save yourself from Hellfire even if it is by giving half a date in charity.'\n",
      "LR: 0.0, RF: 0.0, SVM: 0.0, NN: 0, Target: 1.0\n",
      "Hellfire is surrounded by desires so be careful and donÛªt let your desires control you! #Afterlife\n",
      "LR: 0.0, RF: 0.0, SVM: 0.0, NN: 0, Target: 1.0\n",
      "Hellfire! We donÛªt even want to think about it or mention it so letÛªs not do anything that leads to it #islam!\n",
      "LR: 1.0, RF: 1.0, SVM: 1.0, NN: 1, Target: 0.0\n",
      "#Allah describes piling up #wealth thinking it would last #forever as the description of the people of #Hellfire in Surah Humaza. #Reflect\n",
      "LR: 0.0, RF: 0.0, SVM: 0.0, NN: 0, Target: 1.0\n",
      "The Prophet (peace be upon him) said 'Save yourself from Hellfire even if it is by giving half a date in charity.'\n",
      "LR: 0.0, RF: 0.0, SVM: 0.0, NN: 0, Target: 1.0\n",
      "CLEARED:incident with injury:I-495  inner loop Exit 31 - MD 97/Georgia Ave Silver Spring\n",
      "LR: 1.0, RF: 1.0, SVM: 1.0, NN: 1, Target: 0.0\n",
      "wowo--=== 12000 Nigerian refugees repatriated from Cameroon\n",
      "LR: 1.0, RF: 1.0, SVM: 1.0, NN: 1, Target: 0.0\n",
      "In #islam saving a person is equal in reward to saving all humans! Islam is the opposite of terrorism!\n",
      "LR: 1.0, RF: 1.0, SVM: 1.0, NN: 1, Target: 0.0\n"
     ]
    }
   ],
   "source": [
    "common_incorrects = np.where(\n",
    "    (LR_predictions != targets) &\n",
    "    (RF_predictions != targets) &\n",
    "    (SVM_predictions != targets) &\n",
    "    (NN_predictions != targets)\n",
    ")\n",
    "\n",
    "for idx in common_incorrects[0]:\n",
    "    print(df['text'].iloc[idx])\n",
    "    print(f'LR: {LR_predictions[idx]}, RF: {RF_predictions[idx]}, SVM: {SVM_predictions[idx]}, NN: {NN_predictions[idx]}, Target: {targets[idx]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
